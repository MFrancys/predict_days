{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from operator import getitem\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)\n",
    "\n",
    "import mlflow\n",
    "import mlflow.h2o\n",
    "import mlflow.sklearn\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import h2o\n",
    "from h2o.automl import H2OAutoML\n",
    "h2o.init()\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from src.models.train import train_h2o_models\n",
    "\n",
    "from src.models.constant_models import list_features\n",
    "from src.models.constant_models import list_categories\n",
    "from src.models.constant_models import list_categories_financial_features\n",
    "from src.models.constant_models import list_financial_features\n",
    "from src.models.constant_models import list_categories_purchase_features\n",
    "from src.models.constant_models import list_purchase_features\n",
    "from src.models.constant_models import financial_rating \n",
    "from src.models.constant_models import purchase_rating\n",
    "\n",
    "from src.models.predict import get_kbins_discretizer_probability\n",
    "from src.models.predict import get_h2o_model_object\n",
    "\n",
    "from src.models.qualification_priority import get_deal_qualification\n",
    "\n",
    "from src.connections.manage_connecting_mlflow import send_to_mlflow_metric\n",
    "from src.connections.manage_connecting_mlflow import get_mlflow_experiment\n",
    "\n",
    "from src.models.transform_data import split_data\n",
    "from src.models.transform_data import get_label_encoder\n",
    "from src.models.transform_data import get_label_encoder_features\n",
    "from src.models.transform_data import resample_data\n",
    "from src.models.transform_data import get_famd\n",
    "from src.models.transform_data import get_label_encoder_inverse\n",
    "from src.models.transform_data import get_one_hot_encoder\n",
    "from src.models.transform_data import get_one_hot_encoder_features\n",
    "\n",
    "from src.visualization.graphs import get_famd_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_model_by_set(client, experiment_name, exp_id, list_features_name, list_categories_features, dict_qualification):\n",
    "            \n",
    "    logging.info(\"\"\"One Hot_encoding\"\"\")\n",
    "    df_train_analysis = df_train.copy()\n",
    "    one_hot_enc = get_one_hot_encoder(df_train_analysis, list_categories_features)\n",
    "    df_train_analysis = get_one_hot_encoder_features(one_hot_enc, df_train_analysis, list_features_name, list_categories_features)\n",
    "    df_test_analysis = get_one_hot_encoder_features(one_hot_enc, df_test, list_features_name, list_categories_features)\n",
    "    try:\n",
    "        df_train_analysis = df_train_analysis.drop(columns=\"status_form_NOT_PRE_APPROVED\")\n",
    "        df_test_analysis = df_test_analysis.drop(columns=\"status_form_NOT_PRE_APPROVED\")\n",
    "    except:\n",
    "        pass\n",
    "    mlflow.sklearn.log_model(sk_model=one_hot_enc, artifact_path=\"processig_data/one_hot_enc\")\n",
    "    \n",
    "    logging.info(\"\"\"Training model\"\"\")\n",
    "    ### https://medium.com/@shalinisinghzoots/the-mlflow-tracking-component-is-an-api-and-ui-for-logging-parameters-code-versions-metrics-and-24f078acb55e\n",
    "    train = h2o.H2OFrame.from_python(df_train_analysis)\n",
    "    test = h2o.H2OFrame.from_python(df_test_analysis)\n",
    "    feature_factors = [s for s in df_train_analysis.columns if any(xs in s for xs in list_categories_features)]\n",
    "    for feature in feature_factors:\n",
    "        train[feature] = train[feature].asfactor()\n",
    "        test[feature] = test[feature].asfactor()\n",
    "    train.describe()\n",
    "    model = train_h2o_models(train)\n",
    "\n",
    "    logging.info(\"\"\"Sending metrics and artifacts to MLFlows\"\"\")\n",
    "    y_pred = model.predict(test)[\"predict\"].as_data_frame()\n",
    "    y_test = df_test_analysis[\"approved\"]\n",
    "    send_to_mlflow_metric(mlflow, model, test, y_test, y_pred, MODELS_PATH)\n",
    "    result = model.predict(train)\n",
    "\n",
    "    logging.info(\"\"\"Getting object priorization model\"\"\")\n",
    "    model = get_h2o_model_object(client, exp_id, ROOT_PATH)\n",
    "\n",
    "    logging.info(\"\"\"Getting data preditions\"\"\")\n",
    "    result = model.predict(train)\n",
    "    df_result = result.as_data_frame()\n",
    "    df_result = round(df_result[[\"True\"]].astype(float), 6)\n",
    "    \n",
    "    logging.info(\"\"\"Getting KBinsDiscretizer probability\"\"\") #strategy{‘uniform’, ‘quantile’, ‘kmeans’}, \n",
    "    strategy = \"kmeans\"\n",
    "    kbins_object, probability_bins,qualification_bins = get_kbins_discretizer_probability(df_result, features_name, n_bins=3, strategy=strategy)\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=kbins_object, \n",
    "        artifact_path=\"processig_data/probability_kbins_object\"\n",
    "    )\n",
    "    \n",
    "    data_trans = kbins_object.transform(df_result)\n",
    "    df_data_trans = pd.DataFrame(data=data_trans, columns=[f\"probability_{features_name}\"])\n",
    "    df_data_trans[f\"probability_{features_name}_bin\"] = df_data_trans[f\"probability_{features_name}\"]. \\\n",
    "        map(probability_bins)\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=kbins_object, \n",
    "        artifact_path=\"processig_data/probability_bins\"\n",
    "    )\n",
    "    \n",
    "    df_data_trans[f\"qualification_{features_name}\"] = df_data_trans[f\"probability_{features_name}\"]. \\\n",
    "        map(dict_qualification)\n",
    "\n",
    "    df_data = df_data_trans.copy()\n",
    "\n",
    "    logging.info(\"\"\"Getting and count KBinsDiscretizer probability df\"\"\")\n",
    "\n",
    "    df_data_trans = pd.DataFrame(data=df_data_trans, columns=[f\"probability_{features_name}_bin\"])\n",
    "    df_data_trans[\"counts\"] = 1\n",
    "    df_data_trans = df_data_trans.groupby([f\"probability_{features_name}_bin\" ]).count().reset_index()\n",
    "    fig = go.Figure([go.Bar(x=df_data_trans[f\"probability_{features_name}_bin\"], y=df_data_trans.counts,\n",
    "                            text=df_data_trans.counts,\n",
    "                textposition='auto')])\n",
    "    title=f'PROBABILITY BINS - {features_name}'\n",
    "    fig.update_layout(title=title)\n",
    "    fig.show()\n",
    "    fig.write_html(os.path.join(MODELS_PATH, f\"{title}.html\"))\n",
    "    mlflow.log_artifact(os.path.join(MODELS_PATH, f\"{title}.html\"))\n",
    "    \n",
    "\n",
    " \n",
    "    return df_data, probability_bins "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name = \"financial_features\"\n",
    "list_categories_features = list_categories_financial_features\n",
    "list_features_name = list_financial_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\"\"Getting experiment MLFlow\"\"\")\n",
    "client = MlflowClient()\n",
    "experiment_name = f\"my_priorization_model_data\"\n",
    "exp_id = get_mlflow_experiment(experiment_name, client)\n",
    "run_name = mlflow.start_run(experiment_id=exp_id, run_name = experiment_name)\n",
    "exp_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.getcwd()\n",
    "OUTPUT_PATH = os.path.join(ROOT_PATH, f\"output_{features_name}\")\n",
    "DATA_PATH = os.path.join(OUTPUT_PATH, \"data\")\n",
    "MODELS_PATH = os.path.join(OUTPUT_PATH, \"models\")\n",
    "FILE_PATH = os.path.join(ROOT_PATH, \"output_form.csv\")\n",
    "\n",
    "logging.info(\"\"\"Getting deals information\"\"\")\n",
    "df = pd.read_csv(FILE_PATH).set_index(\"id_row\"). \\\n",
    "    rename(columns={'comimments_net_income': 'comimments_gross_income'})\n",
    "\n",
    "df[\"payment_terms\"] = df[\"payment_terms\"].astype(str)\n",
    "\n",
    "df = df[df[\"net_income\"] < 200000]\n",
    "df = df[~((df[\"created\"] < '2020-05-15 00:00:00.834166') & (df[\"approved\"] == False))]\n",
    "df = df[df.email.str.find(\"@truehome.com.mx\") == -1]\n",
    "df = df[~(df[\"causas_perdida\"].isnull() & (df.approved == False)) | (df.approved == True)]\n",
    "\n",
    "logging.info(\"\"\"Deals Approved: {}\"\"\".format(df[\"approved\"].value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\"\"Splitting data in train set and test set\"\"\")\n",
    "df = df[list_features]\n",
    "y = df[[\"approved\"]]\n",
    "X = df.drop(columns=\"approved\")\n",
    "X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "df_train.to_csv(os.path.join(DATA_PATH, \"df_train_raw.csv\"))\n",
    "mlflow.log_artifact(os.path.join(DATA_PATH, \"df_train_raw.csv\"))\n",
    "\n",
    "df_test.to_csv(os.path.join(DATA_PATH, \"df_test_raw.csv\"))\n",
    "mlflow.log_artifact(os.path.join(DATA_PATH, \"df_test_raw.csv\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\"\"Processing data in train set and test set\"\"\")\n",
    "\n",
    "logging.info(\"\"\"Resampling train data\"\"\")\n",
    "label_encoder, object_default_dict = get_label_encoder(df_train, list_categories)\n",
    "df_train_label = get_label_encoder_features(object_default_dict, df_train, list_categories)\n",
    "\n",
    "X_train = df_train_label[list_features].drop(columns=\"approved\")\n",
    "y_train = df_train_label[list_features][['approved']]\n",
    "\n",
    "df_resampled_train = resample_data(X_train, y_train)\n",
    "df_famd = get_famd(df_resampled_train, list_features)\n",
    "\n",
    "\n",
    "fig = get_famd_graph(df_famd)\n",
    "fig.write_html(os.path.join(DATA_PATH, \"famd_features_file.html\"))\n",
    "mlflow.log_artifact(os.path.join(DATA_PATH, \"famd_features_file.html\"))\n",
    "\n",
    "df_train = get_label_encoder_inverse(df_resampled_train, object_default_dict, list_categories)\n",
    "\n",
    "\n",
    "logging.info(\"\"\"Saving data preprocessing steps\"\"\")\n",
    "data_preprocessing_steps = {\n",
    "    \"data_preprocessing\": {\n",
    "        \"label_features\": {\n",
    "            \"label_encoder\": label_encoder,\n",
    "            \"object_defaultdict\": object_default_dict\n",
    "        },  \n",
    "        #\"one_hot_enc\": one_hot_enc\n",
    "    }\n",
    "}\n",
    "\n",
    "pickle_preprocessing_steps = os.path.join(MODELS_PATH, \"data_preprocessing_steps.pickle\")\n",
    "\n",
    "pickle.dump(\n",
    "    data_preprocessing_steps, \n",
    "    open(pickle_preprocessing_steps , \"wb\")\n",
    ")  \n",
    "mlflow.log_artifact(pickle_preprocessing_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - PURCHASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name = \"purchase_features\"\n",
    "list_categories_features = list_categories_purchase_features\n",
    "list_features_name = list_purchase_features\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment_name = f\"my_priorization_model_with_{features_name}\"\n",
    "exp_id = get_mlflow_experiment(experiment_name, client)\n",
    "\n",
    "logging.info(\"\"\"Getting experiment MLFlow\"\"\")\n",
    "run_name = mlflow.start_run(experiment_id=exp_id, run_name = experiment_name)\n",
    "\n",
    "df_data_trans_purchase, probability_purchase_bins = train_model_by_set(client, experiment_name, exp_id, list_features_name, list_categories_features, purchase_rating)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model - FINANCIAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing data by feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name = \"financial_features\"\n",
    "list_categories_features = list_categories_financial_features\n",
    "list_features_name = list_financial_features\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment_name = f\"my_priorization_model_with_{features_name}\"\n",
    "exp_id = get_mlflow_experiment(experiment_name, client)\n",
    "\n",
    "logging.info(\"\"\"Getting experiment MLFlow\"\"\")\n",
    "run_name = mlflow.start_run(experiment_id=exp_id, run_name = experiment_name)\n",
    "\n",
    "df_data_trans_financial, probability_financial_bins = train_model_by_set(client, experiment_name, exp_id, list_financial_features, list_categories_financial_features, financial_rating)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(\"\"\"Getting experiment MLFlow\"\"\")\n",
    "client = MlflowClient()\n",
    "experiment_name = f\"my_priorization_model_qualification_priority\"\n",
    "exp_id = get_mlflow_experiment(experiment_name, client)\n",
    "run_name = mlflow.start_run(experiment_id=exp_id, run_name = experiment_name)\n",
    "qualification_priority = get_deal_qualification(probability_financial_bins, probability_purchase_bins)\n",
    "\n",
    "mlflow.sklearn.log_model(\n",
    "    sk_model=qualification_priority, \n",
    "    artifact_path=\"qualification_priority\"\n",
    ")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualification_priority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_qualification = pd.concat([df_data_trans_financial, df_data_trans_purchase], axis=1)\n",
    "df_qualification[\"qualification\"] = df_qualification[\"qualification_financial_features\"] + df_qualification[\"qualification_purchase_features\"]\n",
    "df_qualification[\"qualification_priority\"] = df_qualification[\"qualification\"].map(qualification_priority)\n",
    "df_qualification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Confusion Matrix: a table showing correct predictions and types of incorrect predictions.\n",
    "- Precision: the number of true positives divided by all positive predictions. Precision is also called Positive Predictive Value. It is a measure of a classifier’s exactness. Low precision indicates a high number of false positives.\n",
    "- Recall: the number of true positives divided by the number of positive values in the test data. Recall is also called Sensitivity or the True Positive Rate. It is a measure of a classifier’s completeness. Low recall indicates a high number of false negatives.\n",
    "- F1: Score: the weighted average of precision and recall.\n",
    "\n",
    "Compute the balanced accuracy\n",
    "\n",
    "The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "aml = model\n",
    "perf = aml.leader.model_performance(test)\n",
    "#perf.auc()\n",
    "perf\n",
    "\n",
    "m = h2o.get_model(lb[1,\"model_id\"])\n",
    "m.as_data_frame()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
